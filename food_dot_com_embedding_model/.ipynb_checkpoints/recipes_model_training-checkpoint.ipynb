{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis via food.com Word Embedding\n",
    "In this notebook, we load in a word embedding and sentiment analyzer which are both trained specifically on food.com reviews to predict sentiments on reviews for the NYT Salted Tahini Chocolate Chip Cookies recipe (https://cooking.nytimes.com/recipes/1018055-salted-tahini-chocolate-chip-cookies). Our hypothesis is that this would produce a better accuracy that using a generic word embedding and/or sentiment analyzer. \n",
    "\n",
    "The data set used to train these models can be found here: https://www.kaggle.com/irkaal/foodcom-recipes-and-reviews\n",
    "\n",
    "This notebook is part of a collaborative project completed for The Erdos Institute's Code 2021 Data Science Boot camp. The work in this notebook was completed by Anila Yadavalli. The other teammates are Shirley Li, Nida Obtake, and Enkhzaya Enkhtaivan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IT_pnlL5QoM7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anila\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict \n",
    "\n",
    "import spacy \n",
    "import numpy as np\n",
    "import gensim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading in the .csv file which contains the cleaned reviews from the food.com dataset as a dataframe. This is so that we can train the sentiment analyzer later. \n",
    "\n",
    "The cleaning consists of these items:\n",
    "\n",
    "-Removes non-alphabetic characters.\n",
    "\n",
    "-Lemmatizes the words (i.e. 'ran', 'run', 'running', 'runs' all become 'run')\n",
    "\n",
    "-Creates bigrams of common words that appear together (i.e. 'chocolate chip' becomes 'chocolate_chip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "L1o3e_AqRB7H",
    "outputId": "3c6368da-0b40-4c34-a332-54ec2bebe670",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>well than any you can get at a restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>I cut back on the mayo and make up the differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>I think I do something wrong because I could t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>easily the good I have ever have juicy flavorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>an excellent dish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified                                              clean  \n",
       "0  2000-01-25T21:44:00Z          well than any you can get at a restaurant  \n",
       "1  2001-10-17T16:49:59Z  I cut back on the mayo and make up the differe...  \n",
       "2  2000-02-25T09:00:00Z  I think I do something wrong because I could t...  \n",
       "3  2000-03-13T21:15:00Z  easily the good I have ever have juicy flavorf...  \n",
       "4  2000-03-28T12:51:00Z                                  an excellent dish  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_reviews_with_ratings_and_stops.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove any reviews that have a 0-star rating. This is because 0-stars indicate that the reviewer did not select a star rating, so the sentiment for such reviews cannot be classified. This removes about 80,000 reviews from the food.com dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401768\n",
      "1325520\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[df.Rating != 0].reset_index(drop = True)\n",
    "df.sample(20)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assign a numeric score to the sentiment of a review based on the star-rating. Ratings of 4 and 5 are classified as positive and ratings of 1-3 are classified as negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "VGdm0taVRYXn",
    "outputId": "8cc7c2a2-79be-4d3f-8712-d7e0cf60d176",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "      <th>clean</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>well than any you can get at a restaurant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>I cut back on the mayo and make up the differe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>I think I do something wrong because I could t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>easily the good I have ever have juicy flavorf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>an excellent dish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified                                              clean  \\\n",
       "0  2000-01-25T21:44:00Z          well than any you can get at a restaurant   \n",
       "1  2001-10-17T16:49:59Z  I cut back on the mayo and make up the differe...   \n",
       "2  2000-02-25T09:00:00Z  I think I do something wrong because I could t...   \n",
       "3  2000-03-13T21:15:00Z  easily the good I have ever have juicy flavorf...   \n",
       "4  2000-03-28T12:51:00Z                                  an excellent dish   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'] = (df['Rating'] > 3).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning generated empty values for some reviews. We drop those here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fnRFsUp-Ut8q"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our word embedding which was trained on the food.com data. See https://colab.research.google.com/drive/1uqw557Y0l4dOIxO_jTZHUX6Zec9T7Pkl?usp=sharing\n",
    "for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7lkq-FERVTeb"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# have to change the path if connecting to Google Drive.\n",
    "model = Word2Vec.load(\"recipes2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we need to create an array whose columns consist of word vectors for each review, so we need to know the length of the longest review. Alternately, you just pick a maximum length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "876490e49edc40b0ac1708c8884e2141",
      "291edd4c87004b8d9f4f38748092d29e",
      "1081862a6b3746cd8988c806dfc05638",
      "1749ec70f2c64a60820de81414795ba5",
      "340ef84c2dc745beb6c9b7a8b350c2c8",
      "fbdf70f57c7641ce8fbcc16b69a54ebb",
      "b143fef7d3c441beaa2f07746e7bbb2e",
      "bc6709e859c94a459f2c592304e4ea4a"
     ]
    },
    "id": "1fkWmYg9W6oc",
    "outputId": "01b381f5-4159-4687-c5e7-5bd7706c8d86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99895794290f474882d719c60c7db59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max: 1276 index: 1213989\n"
     ]
    }
   ],
   "source": [
    "# find max sentence length\n",
    "max_val = 0\n",
    "idx = 0\n",
    "for i, review in tqdm(enumerate(df['clean'])):\n",
    "      if len(review.split(' ')) > max_val:\n",
    "        max_val = len(review.split(' '))\n",
    "        idx = i\n",
    "print(\"max:\", max_val, \"index:\", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that reads in a review and produces a matrix whose columns are the word vector for each word in the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H2YO7BZpa8Xi"
   },
   "outputs": [],
   "source": [
    "def sentence_to_matrix(sentence, maxlen = max_val, model = model):\n",
    "  #takes in a sentence as a string and outputs a matrix whose rows\n",
    "  #are word vectors for each word in the sentence\n",
    "    sentence_matrix = np.zeros((maxlen, 300))\n",
    "    \n",
    "    #Split the input sentence into words.\n",
    "    sen_len = len(sentence.split(' '))\n",
    "    shift = 0\n",
    "    sen = sentence.split(' ')\n",
    "    \n",
    "    # two loops depending on if the sentence is past the length of maxlen\n",
    "    if sen_len > maxlen:\n",
    "        for i in range(maxlen):\n",
    "            # here sen[::-1] is the reversed list of sen\n",
    "            # we populate the word vector from the back because it needs to be front-padded with zeros\n",
    "            # (otherwise it will just erase everything it learned)\n",
    "            if sen[::-1][i] in model.wv.index_to_key:\n",
    "                sentence_matrix[maxlen - (i+1) + shift,:] = model.wv[sen[::-1][i]]\n",
    "            else:\n",
    "                shift += 1 #skip any words that aren't in the dictionary\n",
    "    else:\n",
    "        for i in range(sen_len):\n",
    "            if sen[::-1][i] in model.wv.index_to_key: \n",
    "                sentence_matrix[maxlen - (i+1) + shift,:] = model.wv[sen[::-1][i]]\n",
    "            else:\n",
    "                shift += 1\n",
    "    \n",
    "    return sentence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the sentiment analyzer! The first step is to create a validation data frame; we chose to use a balanced set of 5-star and 1-star reviews. We used the most extreme ratings to ensure that the sentiment analyzer to make sure that the data was actually positive or negative. Reviews in the 2-4 range could be more neutral. \n",
    "\n",
    "Alternately, one could use 'Sentiment' == 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1315921 1313921 2000\n"
     ]
    }
   ],
   "source": [
    "# create validation set of 1000 positive, 1000 negative\n",
    "df_positive = df.loc[df['Rating']==5]\n",
    "df_negative = df.loc[df['Rating']==1]\n",
    "df_val = pd.concat([df_negative.sample(1000), df_positive.sample(1000)])\n",
    "\n",
    "# ensures training set and validation set are disjoint\n",
    "df_train = df.drop(df_val.index)\n",
    "print(len(df), len(df_train), len(df_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to create training sets that are balanced between negative and positive reviews. We do this in batches of 1000, otherwise the it is too big for a laptop to handle. \n",
    "\n",
    "We create a balanced data set so that the model doesn't 'learn' to just predict positive every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample with 1000 positive reviews and 1000 negative reviews\n",
    "def get_sample(df, size=1000):\n",
    "    X_train = np.zeros((size*2, max_val, 300))\n",
    "    y_train = np.zeros(size*2)\n",
    "    # Could use 'Sentiment' = 0 or 1 instead\n",
    "    df_positive = df.loc[df['Sentiment']==1]\n",
    "    df_negative = df.loc[df['Sentiment']==0]\n",
    "    #creates a data frame containing 50% positive and 50% negative reviews\n",
    "    df_small = pd.concat([df_negative.sample(size), df_positive.sample(size)])\n",
    "    for i in range(len(df_small)):\n",
    "        row = df_small.iloc[i] #loops through df_small and takes out each row as a dictionary\n",
    "        X_train[i] = sentence_to_matrix(row['clean'])\n",
    "        y_train[i] = row['Sentiment']\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_SZSouL_fC6-"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the recurrent neural network (RNN) to train. RNN is a model designed to work with sequence data, which is why can use it to pass in sequences of word vectors (i.e. one review). \n",
    "\n",
    "The output of the model is a single value between 0 and 1 which tells you the probability of a review being positive. \n",
    "\n",
    "We added L2 regularization to the RNN in order to limit the size of the parameters, to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wpHuwrcPfRCg"
   },
   "outputs": [],
   "source": [
    "# added in regularization (L2 like ridge regression!)\n",
    "# Regularizers encourage parameters to stay small using L2 norm.\n",
    "# This just prevents overfitting.\n",
    "\n",
    "RNN = models.Sequential()\n",
    "RNN.add(layers.SimpleRNN(300,\n",
    "                         kernel_regularizer=regularizers.L2(0.01),\n",
    "                         bias_regularizer=regularizers.L2(0.01),\n",
    "                         recurrent_regularizer=regularizers.L2(0.01),\n",
    "                         return_sequences=False))\n",
    "RNN.add(layers.Dense(1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer ```rmsprop``` is a standard optimization algorithm. \n",
    "The loss function ```binary_crossentropy``` is specific to binary classification (e.g. positive/negative)\n",
    "We use ```accuracy``` as a metric to evaluate our model during training. Alternately, you could use ```AUC```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k2c3xs-_fvY7"
   },
   "outputs": [],
   "source": [
    "RNN.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our validation data frame into sentence matrices that we  can input into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = get_sample(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we are actually training the model. For better accuracy, change ```range(6)``` to a higher number, but 6 is the maximum I could do on my laptop without it crashing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLvBILFGhKUv",
    "outputId": "508ccc15-051c-44a1-c711-d0ff12adbf83"
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    X_train, y_train = get_sample(df_train)\n",
    "    RNN.fit(X_train, y_train,\n",
    "                  epochs = 1,\n",
    "                  batch_size = 128,\n",
    "               validation_data=(X_val,y_val))\n",
    "    #print(RNN.predict(X_val[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model is trained, we can save it for future use. \n",
    "RNN.save('rnnmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model on NYT reviews\n",
    "Now we can use this model to predict sentiments on NYT reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "RNN = models.load_model('rnnmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmk</td>\n",
       "      <td>Yum.  These took much longer than 16 minutes t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>yum take long minute cook denver ft altitude g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sonya</td>\n",
       "      <td>If you follow the recipe as written the tahini...</td>\n",
       "      <td>pos</td>\n",
       "      <td>follow recipe write tahini sesame flavour cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KV</td>\n",
       "      <td>I have made these cookies 5 times. My advice i...</td>\n",
       "      <td>pos</td>\n",
       "      <td>cookie time advice recipe say don t tell step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MaryN</td>\n",
       "      <td>I liked this- the tahini is slightly more subt...</td>\n",
       "      <td>pos</td>\n",
       "      <td>like tahini slightly subtle pb cookie combine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maggie B</td>\n",
       "      <td>Used Shaila M's tweaks. Baked first tray strai...</td>\n",
       "      <td>pos</td>\n",
       "      <td>shaila m tweak bake tray straight mix deliciou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user                                            comment sentiment  \\\n",
       "0       lmk  Yum.  These took much longer than 16 minutes t...       pos   \n",
       "1     Sonya  If you follow the recipe as written the tahini...       pos   \n",
       "2        KV  I have made these cookies 5 times. My advice i...       pos   \n",
       "3     MaryN  I liked this- the tahini is slightly more subt...       pos   \n",
       "4  Maggie B  Used Shaila M's tweaks. Baked first tray strai...       pos   \n",
       "\n",
       "                                               clean  \n",
       "0  yum take long minute cook denver ft altitude g...  \n",
       "1  follow recipe write tahini sesame flavour cook...  \n",
       "2  cookie time advice recipe say don t tell step ...  \n",
       "3  like tahini slightly subtle pb cookie combine ...  \n",
       "4  shaila m tweak bake tray straight mix deliciou...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the cleaned tahini dataset here\n",
    "tahini = pd.read_csv(\"tahini_cleaned_comments.csv\")\n",
    "tahini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, since we are testing our model, we assigned sentiments to the NYT reviews manually. Here we are assigning a Sentiment_Score of 0 (negative) or 1 (positive). Leaving neutral comments in, and classifying them as negative was agreed upon for comparison in the context of the overall project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tahini['Sentiment_Score'] = (tahini['sentiment'] == 'pos').astype(int)\n",
    "tahini.dropna(inplace = True).reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921804eb3c614f799b09bb455e384846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max: 57 index: 170\n"
     ]
    }
   ],
   "source": [
    "# Use sentence_to_matrix to convert NYT reviews to vectors\n",
    "# Replace 'tahini' with 'tahini_no_neu' if you decided to leave out neutrals above. \n",
    "\n",
    "tahini_size = len(tahini)\n",
    "tahini_max_val = 0\n",
    "tahini_idx = 0\n",
    "\n",
    "for i, review in tqdm(enumerate(tahini['clean'])):\n",
    "      if len(review.split(' ')) > tahini_max_val:\n",
    "        tahini_max_val = len(review.split(' '))\n",
    "        tahini_idx = i\n",
    "print(\"max:\", tahini_max_val, \"index:\", tahini_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are turning the ```tahini``` data frame into a matrix so we can input it into ```RNN```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((tahini_size, tahini_max_val, 300))\n",
    "y_test = np.zeros(tahini_size)\n",
    "\n",
    "for i in range(len(tahini)):\n",
    "    row = tahini.iloc[i] #loops through tahini and takes out each row as a dictionary\n",
    "    X_test[i] = sentence_to_matrix(row['clean'],tahini_max_val)\n",
    "    y_test[i] = row['Sentiment_Score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do RNN.predict to see the prediction!\n",
    "preds = RNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tahini)):\n",
    "    print(\"P(positive) = \", preds[i], \"; Actual Sentiment = \", tahini.loc[i, 'Sentiment_Score'],\n",
    "         \"\\n Actual comment: \", tahini.loc[i, 'comment'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the confusion matrix to determine how well our model did on the Tahini Cookie data. Note that looking at the accuracy score doesn't tell us the whole story since the data had way more positive reviews than negative review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 79,  32],\n",
       "       [110, 127]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = (preds > 0.5) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, the AUC (area under curve) score looks decent, which tells us that a lower threshold for predicting positive could give us better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5919540229885057, 0.6714562663929753)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_preds), roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we lower the threshold, we sacrifice our accuracy of negatives, for a better accuracy of positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,  44],\n",
       "       [ 89, 148]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = (preds > 0.35) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the process, but drop the neutral sentiment reviews. We see this improves the AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0257de21d44a8984b1e8da5afce974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max: 57 index: 133\n"
     ]
    }
   ],
   "source": [
    "tahini_no_neu = tahini[tahini.sentiment != 'neu'].reset_index(drop = True)\n",
    "\n",
    "tahini_size = len(tahini_no_neu)\n",
    "tahini_max_val = 0\n",
    "tahini_idx = 0\n",
    "\n",
    "for i, review in tqdm(enumerate(tahini_no_neu['clean'])):\n",
    "      if len(review.split(' ')) > tahini_max_val:\n",
    "        tahini_max_val = len(review.split(' '))\n",
    "        tahini_idx = i\n",
    "print(\"max:\", tahini_max_val, \"index:\", tahini_idx)\n",
    "\n",
    "X_test = np.zeros((tahini_size, tahini_max_val, 300))\n",
    "y_test = np.zeros(tahini_size)\n",
    "\n",
    "for i in range(len(tahini_no_neu)):\n",
    "    row = tahini_no_neu.iloc[i] #loops through tahini and takes out each row as a dictionary\n",
    "    X_test[i] = sentence_to_matrix(row['clean'],tahini_max_val)\n",
    "    y_test[i] = row['Sentiment_Score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the confusion matrix tells us that the model predicts negative reviews very well, but there are a lot of false positives.\n",
    "\n",
    "This could mean that according to the model, a user has to really love the cookie recipe for their review to be detected as positive! ðŸ¤·ðŸ½â€â™€ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24,   2],\n",
       "       [110, 127]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = RNN.predict(X_test)\n",
    "\n",
    "y_preds = (preds > 0.5) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score looks much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5741444866920152, 0.8377150275884453)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_preds), roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like above, when we lower the threshold, we sacrifice our accuracy of negatives, for a better accuracy of positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 23,   3],\n",
       "       [ 89, 148]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = (preds > 0.35) # Play around with 0.5 for different accuracy scores. \n",
    "tahini_matrix = confusion_matrix(y_test, y_preds)\n",
    "tahini_matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "recipes_model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1081862a6b3746cd8988c806dfc05638": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbdf70f57c7641ce8fbcc16b69a54ebb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_340ef84c2dc745beb6c9b7a8b350c2c8",
      "value": 1
     }
    },
    "1749ec70f2c64a60820de81414795ba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc6709e859c94a459f2c592304e4ea4a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b143fef7d3c441beaa2f07746e7bbb2e",
      "value": " 1376004/? [00:09&lt;00:00, 149806.12it/s]"
     }
    },
    "291edd4c87004b8d9f4f38748092d29e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "340ef84c2dc745beb6c9b7a8b350c2c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "876490e49edc40b0ac1708c8884e2141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1081862a6b3746cd8988c806dfc05638",
       "IPY_MODEL_1749ec70f2c64a60820de81414795ba5"
      ],
      "layout": "IPY_MODEL_291edd4c87004b8d9f4f38748092d29e"
     }
    },
    "b143fef7d3c441beaa2f07746e7bbb2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc6709e859c94a459f2c592304e4ea4a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbdf70f57c7641ce8fbcc16b69a54ebb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
